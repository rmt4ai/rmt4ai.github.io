---
layout: page
title: Workshop Schedule
permalink: /schedule/
---

## Talks

### Wedenesday, June 5, 2024

#### **8:00 – 8:45** Registration
#### **8:45 – 9:00** Welcoming remarks
#### **9:00 – 10:00** *(Keynote)* 
#### **10:00 – 10:20** Break
#### **10:20 – 10:50** Speaker TBD
#### **10:50 - 11:20** Speaker TBD
#### **11:20 - 11:50** Speaker TBD
#### **11:50 – 1:00** Lunch
#### **1:00 – 1:45** Speaker TBD
#### **1:45 - 2:30** Speaker TBD
#### **2:30 – 2:50** Break
#### **2:50 – 3:35** Speaker TBD 
#### **3:35 - 4:20** Speaker TBD
#### **4:20 – 4:40** Break and poster set up
#### **4:40 – 6:40** Reception and Poster Session

### Thursday, June 6, 2024

#### **8:00 – 9:00** Breakfast
#### **9:00 – 9:30** Speaker TBD
#### **9:30 - 10:00** Speaker TBD
#### **10:00 - 10:30** Speaker TBD
#### **10:30 – 10:45** Break
#### **10:45 – 11:30** Speaker TBD
#### **11:30 - 12:15** Speaker TBD
#### **12:15 – 1:30** Lunch
#### **1:30 – 2:50** Open problem session
#### **2:50 – 3:10** Break
#### **3:10 – 3:55** Speaker TBD
#### **3:55 - 4:40** Speaker TBD
#### **4:40 – 5:30** Small group discussions

### Friday, June 7, 2024

#### **8:00 – 9:00** Breakfast
#### **9:00 – 10:00** *(Keynote)*
#### **10:00 – 10:20** Break

#### **10:20 – 11:05** Panos Stinis

**Title TBD**

*Abstract*: forthcoming.

#### **11:05 - 11:50** [Mert Gürbüzbalaban](https://mert-g.org/) (Rutgers)

**Heavy Tail Phenomenon in Stochastic Gradient Descent**

*Abstract*: Stochastic gradient descent (SGD) methods are workhorse methods for training machine learning models, particularly in deep learning. After presenting numerical evidence demonstrating that SGD iterates with constant step size can exhibit heavy-tailed behavior even when the data is light-tailed, in the first part of the talk, we delve into the theoretical origins of heavy tails in SGD iterations based on analyzing products of random matrices and their connection to various capacity and complexity notions proposed for characterizing SGD's generalization properties in deep learning. Key notions correlating with performance on unseen data include the 'flatness' of the local minimum found by SGD (related to the Hessian eigenvalues), the ratio of step size η to batch size b (controlling stochastic gradient noise magnitude), and the 'tail-index' (which measures the heaviness of the tails of the eigenspectra of the network weights). We argue that these seemingly disparate perspectives on generalization are deeply intertwined. Depending on the Hessian structure at the minimum and algorithm parameter choices, SGD iterates converge to a heavy-tailed stationary distribution. We rigorously prove this claim in linear regression, demonstrating heavy tails and infinite variance in iterates even in simple quadratic optimization with Gaussian data. We further analyze tail behavior with respect to algorithm parameters, dimension, and curvature, providing insights into SGD behavior in deep learning. Experimental validation on synthetic data and neural networks supports our theory. Additionally, we discuss generalizations to decentralized stochastic gradient algorithms and to other popular step size schedules including the cyclic step sizes. In the second part of the talk, we introduce a new class of initialization schemes for fully-connected neural networks that enhance SGD training performance by inducing a specific heavy-tailed behavior in stochastic gradients. Based on joint work with Yuanhan Hu, Umut Simsekli, and Lingjiong Zhu. 

#### **11:50 – 1:00** Lunch

#### **1:00 – 2:30** Ending remarks and small group discussion


## Posters

*Information coming soon*
